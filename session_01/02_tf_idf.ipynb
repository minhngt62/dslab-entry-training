{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "308b3421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "PRJ_ROOT_DIR = os.path.dirname(os.path.abspath(''))\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import defaultdict\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f676007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures\n",
    "NOTE_ROOT_DIR = os.path.abspath('')\n",
    "DATA_DIR = os.path.join(NOTE_ROOT_DIR, \"data\", \"20news-bydate\")\n",
    "CHAPTER_ID = \"02_tf_idf\"\n",
    "IMAGES_PATH = os.path.join(NOTE_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00288a01",
   "metadata": {},
   "source": [
    "## 1 - Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b827aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data\n",
    "def gather_20newsgropups_data():\n",
    "    path = os.path.join(NOTE_ROOT_DIR, \"data\", \"20news-bydate\")\n",
    "    train_dir = os.path.join(path, \"20news-bydate-train\")\n",
    "    test_dir = os.path.join(path, \"20news-bydate-test\")\n",
    "\n",
    "    newsgroup_list = [news_group for news_group in os.listdir(train_dir)]\n",
    "    newsgroup_list.sort()\n",
    "    with open (os.path.join(path, \"stop_words.txt\")) as f:\n",
    "        stop_words = (f.read().splitlines())\n",
    "    stemmer = PorterStemmer()\n",
    "    def collect_data_from(parent_dir, newsgroup_list):\n",
    "        data = []\n",
    "        for group_id, newsgroup in enumerate (newsgroup_list):\n",
    "            label = group_id\n",
    "            dir_path = parent_dir + \"/\" + newsgroup + \"/\"\n",
    "            files = [(filename, dir_path + filename)\n",
    "                    for filename in os.listdir(dir_path)\n",
    "                    if os.path.isfile(dir_path + filename)]\n",
    "            files.sort()\n",
    "            for filename, filepath in files:\n",
    "                    # print(filepath)\n",
    "                    with open(filepath, errors = \"ignore\") as f:\n",
    "                        text = f.read().lower()\n",
    "                        words = [stemmer.stem(word)\n",
    "                                for word in re.split(\"\\W+\", text)\n",
    "                                if word not in stop_words]\n",
    "\n",
    "                        content = \" \".join(words)\n",
    "                        assert len(content.splitlines()) == 1\n",
    "                        data.append(str(label) + \"<fff>\" +\n",
    "                                    filename + \"<fff>\" + content)\n",
    "        return data\n",
    "    train_data = collect_data_from(parent_dir= train_dir, newsgroup_list = newsgroup_list)\n",
    "    test_data = collect_data_from(parent_dir= test_dir, newsgroup_list = newsgroup_list)\n",
    "    full_data = train_data + test_data\n",
    "    with open (os.path.join(path, \"20news-train-processed.txt\"), \"w\") as f:\n",
    "        f.write('\\n'.join(train_data))\n",
    "    with open (os.path.join(path, \"20news-test-processed.txt\"), \"w\") as f:\n",
    "        f.write('\\n'.join(test_data))\n",
    "    with open (os.path.join(path, \"20news-full-processed.txt\"), \"w\") as f:\n",
    "        f.write('\\n'.join(full_data))\n",
    "\n",
    "gather_20newsgropups_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54c60fe",
   "metadata": {},
   "source": [
    "## 2 - TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b22419",
   "metadata": {},
   "source": [
    "TF-IDF is scored between 0 and 1. The higher the numerical weight value, the rarer the term. The smaller the weight, the more common the term.\n",
    "\n",
    "**TF (term frequency) - Example**\n",
    "\n",
    "The TF (term frequency) of a word is the frequency of a word (i.e., number of times it appears) in a document. When you know TF, you’re able to see if you’re using a term too much or too little. When a 100-word document contains the term “cat” 12 times, the TF for the word ‘cat’ is\n",
    "\n",
    "$TF(cat) = \\frac{12}{100}$ (i.e. $0.12$)\n",
    "\n",
    "**IDF (inverse document frequency) - Example**\n",
    "\n",
    "The IDF (inverse document frequency) of a word is the measure of how significant that term is in the whole corpus (a body of documents).\n",
    "\n",
    "Let’s say the size of the corpus is 10,000,000 million documents. If we assume there are 0.3 million documents that contain the term “cat”, then the IDF (i.e. log {DF}) is given by the total number of documents (10,000,000) divided by the number of documents containing the term “cat” (300,000).\n",
    "\n",
    "$IDF (cat) = \\log(\\frac{10,000,000}{300,000}) = 1.52$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a05bd4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 14231\n"
     ]
    }
   ],
   "source": [
    "# Suppose the data in the train set is combined by total files\n",
    "def generate_vocabulary(data_path):\n",
    "    # compute inverse document frequency\n",
    "    def compute_idf(df, corpus_size):\n",
    "        # df -> document frequency\n",
    "        assert df > 0\n",
    "        return np.log10(corpus_size * 1. / df)\n",
    "        \n",
    "    with open (data_path, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "    \n",
    "    \"\"\"\n",
    "    doc_count is the list with keys are distince word in the\n",
    "    vocabulary and the values is the number of document\n",
    "    containing that word in the corpus\n",
    "    \"\"\"\n",
    "    doc_count = defaultdict(int)\n",
    "    corpus_size = len(lines)\n",
    "\n",
    "    for line in lines:\n",
    "        features = line.split('<fff>')\n",
    "        text = features[-1]\n",
    "        words = list(set(text.split()))\n",
    "        for word in words:\n",
    "            doc_count[word] += 1\n",
    "    \n",
    "    \"\"\"\n",
    "    words_idfs is list containing pairs of values - word and idf of that word\n",
    "    under condition that the frequency is larger than 10\n",
    "    \"\"\"\n",
    "    words_idfs = [(word, compute_idf(document_freq, corpus_size))\n",
    "                for word, document_freq in zip(doc_count.keys(), doc_count.values())\n",
    "                if document_freq > 10 and not word.isdigit()]\n",
    "    \n",
    "    data_path = os.path.join(NOTE_ROOT_DIR, \"data\", \"20news-bydate\")\n",
    "    words_idfs.sort(key=lambda x: -x[1])\n",
    "    print(\"Vocabulary size {}\". format(len(words_idfs)))\n",
    "    with open(os.path.join(data_path, \"words_idfs.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join([word + \"<fff>\" + str(idf) for word, idf in words_idfs]))\n",
    "\n",
    "generate_vocabulary(os.path.join(DATA_DIR, \"20news-full-processed.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "954fab00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0-th/18846 documents\n",
      "Processing 2000-th/18846 documents\n",
      "Processing 4000-th/18846 documents\n",
      "Processing 6000-th/18846 documents\n",
      "Processing 8000-th/18846 documents\n",
      "Processing 10000-th/18846 documents\n",
      "Processing 12000-th/18846 documents\n",
      "Processing 14000-th/18846 documents\n",
      "Processing 16000-th/18846 documents\n",
      "Processing 18000-th/18846 documents\n",
      "Already calculated tf-idf values of 18846 documents in the corpus. File was written and saved\n"
     ]
    }
   ],
   "source": [
    "# Compute TF-IDF\n",
    "def get_tf_idf(data_path):\n",
    "    with open (os.path.join(DATA_DIR, \"words_idfs.txt\")) as f:\n",
    "        word_idfs = [(line.split(\"<fff>\")[0], float(line.split(\"<fff>\")[1]))  \n",
    "                    for line in f.read().splitlines()]\n",
    "\n",
    "        idfs = dict(word_idfs)\n",
    "        word_IDs = dict([(word, index) for index, (word, idf) in enumerate(word_idfs)])\n",
    "    \n",
    "    with open (data_path) as f:\n",
    "        documents = [\n",
    "            (int(line.split(\"<fff>\")[0]),\n",
    "            int(line.split(\"<fff>\")[1]),\n",
    "            line.split(\"<fff>\")[2])\n",
    "            for line in f.read().splitlines()]\n",
    "        total_doc_num = len(documents)\n",
    "        \n",
    "    data_tf_idf = []\n",
    "    for i, document in enumerate(documents):\n",
    "        if i % 2000 == 0:\n",
    "            print(\"Processing {i}-th/{total_doc_num} documents\".format(i = i, total_doc_num = total_doc_num))\n",
    "        # unpack document\n",
    "        label, doc_id, text = document\n",
    "        words = [word for word in text.split() if word in idfs]\n",
    "        word_set = list(set(words))\n",
    "\n",
    "        max_term_freq = max([words.count(word) for word in word_set])\n",
    "        words_tf_idf = []\n",
    "        sum_squares = 0.0\n",
    "        \n",
    "        for word in word_set:\n",
    "            term_freq = words.count(word)\n",
    "\n",
    "            tf_idf_value = term_freq * 1. / max_term_freq * idfs[word]\n",
    "            words_tf_idf.append((word_IDs[word], tf_idf_value))\n",
    "            sum_squares = sum_squares + tf_idf_value ** 2\n",
    "        \n",
    "        words_tfidfs_normalized = [str(index)+\":\"+ str(tf_idf_value / np.sqrt(sum_squares))\n",
    "                                for index, tf_idf_value in words_tf_idf]\n",
    "        spare_rep = \" \".join(words_tfidfs_normalized)\n",
    "        data_tf_idf.append((label, doc_id, spare_rep))\n",
    "    with open(os.path.join(DATA_DIR, \"data_tf_idf.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join([str(label) + \"<fff>\" + str(doc_id) + \"<fff>\" + spare_rep \\\n",
    "                            for label, doc_id, spare_rep in data_tf_idf]))\n",
    "    print(\"Already calculated tf-idf values of {} documents in the corpus. File was written and saved\".format(total_doc_num))\n",
    "\n",
    "get_tf_idf(os.path.join(DATA_DIR, \"20news-full-processed.txt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
