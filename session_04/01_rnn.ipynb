{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fc00fcc",
   "metadata": {},
   "source": [
    "# RECURRENT NEURAL NETWORKS\n",
    "A notebook about RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "540ba861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\CORE\\anaconda3\\envs\\dslab_training\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Tensorflow is required\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Common imports\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "PRJ_ROOT_DIR = os.path.dirname(os.path.abspath(''))\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f68d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures\n",
    "NOTE_ROOT_DIR = os.path.abspath('')\n",
    "DATA_DIR = os.path.join(NOTE_ROOT_DIR, \"data\", \"20news-bydate\")\n",
    "CHAPTER_ID = \"01_rnn\"\n",
    "IMAGES_PATH = os.path.join(NOTE_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf0380",
   "metadata": {},
   "source": [
    "## 1 - Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062baaec",
   "metadata": {},
   "source": [
    "### 1.1 - Generate the Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03f181ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to build dictionary\n",
    "def gen_data_and_vocab():\n",
    "    def collect_data_from(parent_path, newsgroup_list, word_count=None):\n",
    "        data = []\n",
    "        for group_id, newsgroup in enumerate(newsgroup_list):\n",
    "            dir_path = os.path.join(parent_path, newsgroup)\n",
    "            files = [(filename, os.path.join(dir_path, filename)) for filename in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, filename))]\n",
    "            files.sort()\n",
    "            label = group_id\n",
    "            print(\"Processing: {}-{}\".format(group_id, newsgroup))\n",
    "            \n",
    "            for filename, filepath in files:\n",
    "                with open(filepath) as f:\n",
    "                    text = f.read().lower()\n",
    "                    words = re.split(\"\\W+\", text)\n",
    "                    if word_count is not None:\n",
    "                        for word in words:\n",
    "                            word_count[word] += 1\n",
    "                    content = \" \".join(words)\n",
    "                    assert len(content.splitlines()) == 1\n",
    "                    data.append(str(label) + \"<fff>\" + filename + \"<fff>\" + content)\n",
    "        return data\n",
    "    \n",
    "    word_count = defaultdict(int)\n",
    "    parts = [os.path.join(DATA_DIR, dir_name) for dir_name in os.listdir(DATA_DIR) if not os.path.isfile(os.path.join(DATA_DIR, dir_name))]\n",
    "    \n",
    "    train_path, test_path = (parts[0], parts[1]) if \"train\" in parts[0] else (parts[1], parts[0])\n",
    "    \n",
    "    newsgroup_list = [newsgroup for newsgroup in os.listdir(train_path)]\n",
    "    newsgroup_list.sort()\n",
    "    \n",
    "    train_data = collect_data_from(\n",
    "        parent_path=train_path,\n",
    "        newsgroup_list=newsgroup_list,\n",
    "        word_count=word_count\n",
    "    )\n",
    "    vocab = [word for word, freq in zip(word_count.keys(), word_count.values()) if freq > 10]\n",
    "    vocab.sort()\n",
    "    with open(os.path.join(DATA_DIR, \"w2v\", \"vocab-raw.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(vocab))\n",
    "    \n",
    "    test_data = collect_data_from(\n",
    "        parent_path=test_path,\n",
    "        newsgroup_list=newsgroup_list\n",
    "    )\n",
    "    with open(os.path.join(DATA_DIR, \"w2v\", \"20news_train_raw.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(train_data))\n",
    "    with open(os.path.join(DATA_DIR, \"w2v\", \"20news_test_raw.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(test_data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07d21931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 0-alt.atheism\n",
      "Processing: 1-comp.graphics\n",
      "Processing: 2-comp.os.ms-windows.misc\n",
      "Processing: 3-comp.sys.ibm.pc.hardware\n",
      "Processing: 4-comp.sys.mac.hardware\n",
      "Processing: 5-comp.windows.x\n",
      "Processing: 6-misc.forsale\n",
      "Processing: 7-rec.autos\n",
      "Processing: 8-rec.motorcycles\n",
      "Processing: 9-rec.sport.baseball\n",
      "Processing: 10-rec.sport.hockey\n",
      "Processing: 11-sci.crypt\n",
      "Processing: 12-sci.electronics\n",
      "Processing: 13-sci.med\n",
      "Processing: 14-sci.space\n",
      "Processing: 15-soc.religion.christian\n",
      "Processing: 16-talk.politics.guns\n",
      "Processing: 17-talk.politics.mideast\n",
      "Processing: 18-talk.politics.misc\n",
      "Processing: 19-talk.religion.misc\n",
      "Processing: 0-alt.atheism\n",
      "Processing: 1-comp.graphics\n",
      "Processing: 2-comp.os.ms-windows.misc\n",
      "Processing: 3-comp.sys.ibm.pc.hardware\n",
      "Processing: 4-comp.sys.mac.hardware\n",
      "Processing: 5-comp.windows.x\n",
      "Processing: 6-misc.forsale\n",
      "Processing: 7-rec.autos\n",
      "Processing: 8-rec.motorcycles\n",
      "Processing: 9-rec.sport.baseball\n",
      "Processing: 10-rec.sport.hockey\n",
      "Processing: 11-sci.crypt\n",
      "Processing: 12-sci.electronics\n",
      "Processing: 13-sci.med\n",
      "Processing: 14-sci.space\n",
      "Processing: 15-soc.religion.christian\n",
      "Processing: 16-talk.politics.guns\n",
      "Processing: 17-talk.politics.mideast\n",
      "Processing: 18-talk.politics.misc\n",
      "Processing: 19-talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "# Build a dictionary for the data\n",
    "gen_data_and_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d023f54",
   "metadata": {},
   "source": [
    "### 1.2 - Encoding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56a94b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to encode the data\n",
    "unknown_ID = 0\n",
    "padding_ID = 1\n",
    "MAX_SENTENCE_LENGTH = 500\n",
    "\n",
    "train_data_path = os.path.join(DATA_DIR, \"w2v\", \"20news_train_raw.txt\")\n",
    "test_data_path = os.path.join(DATA_DIR, \"w2v\", \"20news_test_raw.txt\")\n",
    "vocab_path = os.path.join(DATA_DIR, \"w2v\", \"vocab-raw.txt\")\n",
    "\n",
    "def encode_data(data_path, vocab_path):\n",
    "    with open(vocab_path) as f:\n",
    "        vocab = dict([(word, word_ID + 2)\n",
    "                      for word_ID, word in enumerate(f.read().splitlines())])\n",
    "    with open(data_path) as f:\n",
    "        documents = [(line.split('<fff>')[0], line.split('<fff>')[1], line.split('<fff>')[2])\n",
    "                     for line in f.read().splitlines()]\n",
    "\n",
    "    encoded_data = []\n",
    "    for document in documents:\n",
    "        label, doc_id, text = document\n",
    "        words = text.split()[:MAX_SENTENCE_LENGTH]\n",
    "        sentence_length = len(words)\n",
    "        encoded_text = []\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                encoded_text.append(str(vocab[word]))\n",
    "            else:\n",
    "                encoded_text.append(str(unknown_ID))\n",
    "\n",
    "        if len(words) < MAX_SENTENCE_LENGTH:\n",
    "            num_padding = MAX_SENTENCE_LENGTH - len(words)\n",
    "            for _ in range(num_padding):\n",
    "                encoded_text.append(str(padding_ID))\n",
    "\n",
    "        encoded_data.append(str(label) + '<fff>' + str(doc_id) + '<fff>'\n",
    "                            + str(sentence_length) + '<fff>' + ' '.join(encoded_text))\n",
    "\n",
    "    dir_name = '\\\\'.join(data_path.split('\\\\')[:-1])\n",
    "    file_name = '-'.join(data_path.split('\\\\')[-1].split('-')[:-1]) + '-encoded.txt'\n",
    "    with open(dir_name + '\\\\' + file_name, 'w') as f:\n",
    "        f.write('\\n'.join(encoded_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c99ff0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the data\n",
    "encode_data(train_data_path, vocab_path)\n",
    "encode_data(test_data_path, vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90fedfb",
   "metadata": {},
   "source": [
    "## 2 - RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e84ec177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup RNN training\n",
    "from models import DataReader, RNN\n",
    "loss_report = []\n",
    "accuracy_report = []\n",
    "\n",
    "def train_and_evaluate_RNN():\n",
    "    with open(os.path.join(DATA_DIR, \"w2v\", \"vocab-raw.txt\")) as f:\n",
    "      vocab_size = len(f.read().splitlines())\n",
    "\n",
    "    tf.set_random_seed(2021)\n",
    "    rnn = RNN(\n",
    "      vocab_size=vocab_size,\n",
    "      embedding_size=300,\n",
    "      lstm_size=50,\n",
    "      batch_size=50\n",
    "    )\n",
    "    predicted_labels, loss = rnn.build_graph()\n",
    "    train_op = rnn.trainer(loss=loss, learning_rate=0.01)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        train_data_reader = DataReader(\n",
    "            data_path=os.path.join(DATA_DIR, \"w2v\", \"20news_train_encoded.txt\"),\n",
    "            batch_size=50,\n",
    "        )\n",
    "        test_data_reader = DataReader(\n",
    "            data_path=os.path.join(DATA_DIR, \"w2v\", \"20news_train_encoded.txt\"),\n",
    "            batch_size=50,\n",
    "        )\n",
    "        step = 0\n",
    "        MAX_STEP = 3000\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        while step < MAX_STEP:\n",
    "            next_train_batch = train_data_reader.next_batch()\n",
    "            train_data, train_labels, train_sentence_lengths, train_final_tokens = next_train_batch\n",
    "            plabels_eval, loss_eval, _ = sess.run(\n",
    "                [predicted_labels, loss, train_op],\n",
    "                feed_dict={\n",
    "                    rnn._data: train_data,\n",
    "                    rnn._labels: train_labels,\n",
    "                    rnn._sentence_lengths: train_sentence_lengths,\n",
    "                    rnn._final_tokens: train_final_tokens\n",
    "                }\n",
    "            )\n",
    "            step += 1\n",
    "            if step % 20 == 0:\n",
    "              loss_report.append(loss_eval)\n",
    "              print('loss: {}'.format(loss_eval))\n",
    "            if train_data_reader._batch_id == 0:\n",
    "              num_true_preds = 0\n",
    "              while True:\n",
    "                next_test_batch = test_data_reader.next_batch()\n",
    "                test_data, test_labels, test_sentence_lengths, test_final_tokens = next_test_batch\n",
    "                test_plabels_eval = sess.run(\n",
    "                    predicted_labels,\n",
    "                    feed_dict={\n",
    "                        rnn._data: test_data,\n",
    "                        rnn._labels: test_labels,\n",
    "                        rnn._sentence_lengths: test_sentence_lengths,\n",
    "                        rnn._final_tokens: test_final_tokens\n",
    "                    }\n",
    "                )\n",
    "                matches = np.equal(test_plabels_eval, test_labels)\n",
    "                num_true_preds += np.sum(matches.astype(float))\n",
    "\n",
    "                if test_data_reader._batch_id == 0:\n",
    "                  break\n",
    "\n",
    "              accuracy_report.append(num_true_preds * 100. / test_data_reader._size)        \n",
    "              print('Epoch: {}'.format(train_data_reader._num_epoch))\n",
    "              print('Accuracy on test data: {}'.format(num_true_preds * 100. / test_data_reader._size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8cfe90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\HUST\\BKAI\\DSLab\\Entrance Training\\dslab-training\\session_04\\models\\rnn.py:51: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From D:\\CORE\\anaconda3\\envs\\dslab_training\\lib\\site-packages\\keras\\layers\\rnn\\legacy_cells.py:797: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From D:\\CORE\\anaconda3\\envs\\dslab_training\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "loss: 0.5393661260604858\n",
      "loss: 5.971559047698975\n",
      "loss: 2.3458948135375977\n",
      "loss: 4.631789684295654\n",
      "loss: 2.9802987575531006\n",
      "loss: 3.5584280490875244\n",
      "loss: 4.6480231285095215\n",
      "Epoch: 1\n",
      "Accuracy on test data: 8.346666666666666\n",
      "loss: 3.0219240188598633\n",
      "loss: 2.3509011268615723\n",
      "loss: 2.212888717651367\n",
      "loss: 2.0006773471832275\n",
      "loss: 1.8756399154663086\n",
      "loss: 1.7114988565444946\n",
      "loss: 1.641830325126648\n",
      "loss: 1.1005499362945557\n",
      "Epoch: 2\n",
      "Accuracy on test data: 90.48\n"
     ]
    }
   ],
   "source": [
    "# Train RNN\n",
    "train_and_evaluate_RNN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
